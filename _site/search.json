[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html",
    "title": "Fraud Detection",
    "section": "",
    "text": "This data analytics project revolves around enhancing fraud detection in online financial transactions, a critical business use case vital for both consumers and businesses. The significance lies in the escalating threat of fraudulent activities within e-commerce, leading to substantial financial losses for businesses and potential security risks for consumers.\nOur primary focus is to leverage a comprehensive dataset sourced from Vesta Corporation, a leading payment service company, to develop and refine machine learning models aimed at accurately distinguishing between legitimate and fraudulent transactions.\nThe original evaluation metric for this Kaggle competition is ROC AUC; however, due to the imbalance data problem (shown in isFruad section), I think it’s better to use Precision-Recall Curve AUC as the evaluation metric. Let’s talk about why.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROC curve uses True Positive Rate (TPR) and True Negative Rate (TNR) as y-axis and x-axis.\nTPR indicates the model’s ability to distinguish postive data; TNR indicates the model’s ability to distinguish negative data. Thus, ROC curve shows the overall ability to distinguish postive and negative classes.\nHowever, in imbalance dataset with small amount of TP, TNR is not what we care about.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision-Recall curve uses Precision and Recall as y-axis and x-axis.\nRecall indicates the model’s ability to distinguish postive data; Precision indicates how many of the predicted positive cases are actually positive. Thus, Precision-Recall curve shows the overall ability to find as many as positive cases and also with a precise prediction.\nGiven our imbalanced dataset with a small number of TP and considering the real-world scenario of monitoring fraudulent transactions, utilizing the Precision-Recall curve would be prudent as it specifically emphasizes TP."
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-transaction",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-transaction",
    "title": "Fraud Detection",
    "section": "6-1. EDA-Transaction",
    "text": "6-1. EDA-Transaction\n\n\nCode\nresumetable(df_trans, df_trans.columns.tolist())[:10]\n\n\nDataset Shape: (590540, 394)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      TransactionID\n      int32\n      0\n      590540\n      2987000\n      2987001\n      2987002\n    \n    \n      1\n      isFraud\n      int8\n      0\n      2\n      0\n      0\n      0\n    \n    \n      2\n      TransactionDT\n      int32\n      0\n      573349\n      86400\n      86401\n      86469\n    \n    \n      3\n      TransactionAmt\n      float16\n      0\n      8195\n      68.5\n      29.0\n      59.0\n    \n    \n      4\n      ProductCD\n      object\n      0\n      5\n      W\n      W\n      W\n    \n    \n      5\n      card1\n      int16\n      0\n      13553\n      13926\n      2755\n      4663\n    \n    \n      6\n      card2\n      float16\n      8933\n      500\n      NaN\n      404.0\n      490.0\n    \n    \n      7\n      card3\n      float16\n      1565\n      114\n      150.0\n      150.0\n      150.0\n    \n    \n      8\n      card4\n      object\n      1577\n      4\n      discover\n      mastercard\n      visa\n    \n    \n      9\n      card5\n      float16\n      4259\n      119\n      142.0\n      102.0\n      166.0\n    \n  \n\n\n\n\n\nTransactionID\nTransactionID has 590540 unique values, which is the same as df_trans’s rows, so that we can confirm TransactionID is a valid primary key.\n\n\nisFraud\nThe fraudulent transactions, both in count and total amount, represent only a small portion of the overall transactions. This suggests an imbalance in the data, which is an issue we should consider when building models.\n\n\nCode\ndf_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Count of Transaction\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"Transaction Amount\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()\n\n\n\n\n\n\n\nTransactionDT\n\nMin Date: 2017-12-02 00:00:00, Max Date: 2018-06-01 23:58:51\nThe range of TransactionDT is about half a year\nFraudulent transactions are likely associated with Hours\n\n\n\nCode\nresumetable(df_trans, [\"TransactionDT\"])\n\n\nDataset Shape: (590540, 394)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      TransactionDT\n      int32\n      0\n      573349\n      86400\n      86401\n      86469\n    \n  \n\n\n\n\nLooking at the first, second and third value, it appears that the TransactionDT is structured in a format resembling a time delta measured in seconds. Let’s delve deeper into this for further exploration.\n\n\nCode\nTransactionDT_min = df_trans['TransactionDT'].min()\nTransactionDT_max = df_trans['TransactionDT'].max()\nconvert_to_days = 24*60*60\nTransactionDT_range = (TransactionDT_max-TransactionDT_min)/convert_to_days\nprint(f\"Min TransactionDT:{TransactionDT_min}; Max TransactionDT:{TransactionDT_max}\")\nprint(f\"TransactionDT Range (days):{TransactionDT_range}\")\nprint(\"The range of TransactionDT is about half a year.\")\n\n\nMin TransactionDT:86400; Max TransactionDT:15811131\nTransactionDT Range (days):181.99920138888888\nThe range of TransactionDT is about half a year.\n\n\n\n\nCode\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day\n\nmin_date, max_date = df_trans[\"Date\"].min(), df_trans[\"Date\"].max()\nprint(f\"Min Date: {min_date}, Max Date: {max_date}\")\n\n\nMin Date: 2017-12-02 00:00:00, Max Date: 2018-06-01 23:58:51\n\n\n\n\nCode\nplt.figure(figsize=(16, 6))\n\n# Create subplots with shared y-axis\nax1 = plt.subplot(131)\nax1.plot(df_trans.groupby('_Days').mean()['isFraud'])\nax1.set_xlabel('Day of Month')\nax1.set_ylabel('Fraction of fraudulent transactions')\n\nax2 = plt.subplot(132, sharey=ax1)\nax2.plot(df_trans.groupby('_Weekdays').mean()['isFraud'])\nax2.set_xlabel('Day of week')\nax2.set_ylabel('Fraction of fraudulent transactions')\n\nax2 = plt.subplot(133, sharey=ax1)  # Share y-axis with ax1\nax2.plot(df_trans.groupby('_Hours').mean()['isFraud'])\nax2.set_xlabel('Hour')\nax2.set_ylabel('Fraction of fraudulent transactions')\n\nplt.suptitle(\"Fraudulent Transactions Distribution by Day of Month/ Day of week/ Hour\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\nIt appears that fraudulent transactions are more closely associated with the hour of the day rather than the day of the week. We can consider this as one of the feature for our model.\n\n\nTransactionAmt\n\nThe distribution of TransactionAmt is quite skewed.\nAfter log transfomration, fraud’s TransactionAmt is more spread out between Q1 and Q3.\nAfter log transformation, Not Fruad has more outliers.\n\n\n\nCode\ndf_trans['TransactionAmt'].plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Transaction Amt')\nplt.show()\n\n\n\n\n\nThe distribution of TransactionAmt is quite skewwed, so let’s convert it with log transformation\n\n\nCode\ndf_trans['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()\n\n\n\n\n\n\n\nCode\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\ndf_trans.loc[df_trans['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Fraud',\n          color=color_pal[1],\n          xlim=(-3, 10),\n         ax= ax1)\ndf_trans.loc[df_trans['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Not Fraud',\n          color=color_pal[2],\n          xlim=(-3, 10),\n         ax=ax2)\ndf_trans.loc[df_trans['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Fraud',\n          color=color_pal[1],\n         ax= ax3)\ndf_trans.loc[df_trans['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Not Fraud',\n          color=color_pal[2],\n         ax=ax4)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Apply log transformation to transaction amounts\ndf_trans['Log_TransactionAmt'] = np.log(df_trans['TransactionAmt'] + 1)  # Adding 1 to avoid log(0)\n\nplt.figure(figsize=(8, 6))\n\nsns.boxplot(x='isFraud', y='Log_TransactionAmt', data=df_trans)\nplt.title('Log Transaction Amount Distribution - Not Fraud vs Fraud')\nplt.xlabel('Fraud Status')\nplt.ylabel('Log Transaction Amount')\n\nplt.show()\n\n\n\n\n\n\n\nisFraud & TransactionDT & TransactionAmt\n\nfraud rate occurs between 4 am to 11 am, characterized by fewer transactions and lower transaction amounts.\n\n\n\nCode\nploting_cnt_amt(df_trans, '_Hours')\n\n\n\n\n\nWhen initially analyzing TransactionDT in the previous section, we observed variations in the fraud rate across different hours. Subsequently, examining the two charts above, we identify that the fraud rate occurs between 4 am to 11 am, characterized by fewer transactions and lower transaction amounts. A plausible business hypothesis could suggest that these types of fraud might originate overseas, potentially in different time zones from the local time.\n\n\nisFraud & TransactionDT & ProductCD\n\nProduct C has the highest fraud rate, which is 6 times than Product W.\nThe main product W has the lowest fraud rate.\n\n\n\nCode\nresumetable(df_trans, ['ProductCD'])\n\n\nDataset Shape: (590540, 399)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      ProductCD\n      object\n      0\n      5\n      W\n      W\n      W\n    \n  \n\n\n\n\n\n\nCode\nploting_cnt_amt(df_trans, 'ProductCD')\n\n\n\n\n\n\n\nCard1-6\n\ncard4 (issuer company):\n\nVisa and Mastercard are the two main issuer companies and these two companies have similar fraudd rate.\nDiscover has the highest fraud rate.\n\ncard6 (card type):\n\ndebit and credit card are the two main types.\ncredit card is less common but with higher fraud rate.\n\n\n\n\nCode\nresumetable(df_trans, ['card1', 'card2', 'card3','card4', 'card5', 'card6'])\n\n\nDataset Shape: (590540, 399)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      card1\n      int16\n      0\n      13553\n      13926\n      2755\n      4663\n    \n    \n      1\n      card2\n      float16\n      8933\n      500\n      NaN\n      404.0\n      490.0\n    \n    \n      2\n      card3\n      float16\n      1565\n      114\n      150.0\n      150.0\n      150.0\n    \n    \n      3\n      card4\n      object\n      1577\n      4\n      discover\n      mastercard\n      visa\n    \n    \n      4\n      card5\n      float16\n      4259\n      119\n      142.0\n      102.0\n      166.0\n    \n    \n      5\n      card6\n      object\n      1571\n      4\n      credit\n      credit\n      debit\n    \n  \n\n\n\n\nOut of the provided variables, four are numeric, and two are categorical. As the dataset lacks explicit definitions for the numeric variables, let’s concentrate on the categorical ones, which we can infer likely represent the card issuer company and card type.\n\n\nCode\ncalculate_missing_percentage(df_trans, 'card4')\ndf_trans.card4 = df_trans.card4.fillna('NAN')\n\n\nMissing value percentage for card4: 0.27%\n\n\n\n\nCode\nploting_cnt_amt(df_trans, 'card4')\n\n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_trans, 'card6')\ndf_trans.card6 = df_trans.card6.fillna('NAN')\n\n\nMissing value percentage for card6: 0.27%\n\n\n\n\nCode\nploting_cnt_amt(df_trans, 'card6')\n\n\n\n\n\n\n\naddress\n\naddr1:\n\nSome addr1 have lower transaction and transaction amount but with higher fraud rate, which may imply addr1 is a factor of fraud rate.\nMissing values of addr1 have much higher fraud rate, which may be a good indicator for fraud.\n\naddr2:\n\n99% is the same value, which indicates that it may not be a meaningful variable to predict fraud.\n\n\n\n\nCode\nresumetable(df_trans, ['addr1', 'addr2'])\n\n\nDataset Shape: (590540, 399)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      addr1\n      float16\n      65706\n      332\n      315.0\n      325.0\n      330.0\n    \n    \n      1\n      addr2\n      float16\n      65706\n      74\n      87.0\n      87.0\n      87.0\n    \n  \n\n\n\n\naddr1 and addr2 should be categorical, so let’s transform them first.\n\n\nCode\ndf_trans['addr1'] = df_trans['addr1'].astype('object')\ndf_trans['addr2'] = df_trans['addr2'].astype('object')\nresumetable(df_trans, ['addr1', 'addr2'])\n\n\nDataset Shape: (590540, 399)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      addr1\n      object\n      65706\n      332\n      315.0\n      325.0\n      330.0\n    \n    \n      1\n      addr2\n      object\n      65706\n      74\n      87.0\n      87.0\n      87.0\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_trans, 'addr1')\ncalculate_missing_percentage(df_trans, 'addr2')\n\n\nMissing value percentage for addr1: 11.13%\nMissing value percentage for addr2: 11.13%\n\n\nLet’s fill missing values with “NAN”\n\n\nCode\ndf_trans.addr1 = df_trans.addr1.fillna('NAN')\ndf_trans.addr2 = df_trans.addr2.fillna('NAN')\n\n\n\n\nCode\nresult = df_trans.groupby(['addr1'])['addr1'].count().sort_values(ascending=False)\ntotal_count = result.sum()\nresult_percentage = (result / total_count) * 100\nresult_percentage\n\n\naddr1\nNAN      11.126427\n299.0     7.846209\n325.0     7.239306\n204.0     7.115521\n264.0     6.751448\n           ...    \n425.0     0.000169\n227.0     0.000169\n232.0     0.000169\n417.0     0.000169\n427.0     0.000169\nName: addr1, Length: 333, dtype: float64\n\n\nThere are 332 unique values, let’s focus on addr1 that appears more than 1%\n\n\nCode\nindices = result_percentage[result_percentage > 1].index\nselected_rows = df_trans.loc[df_trans.addr1.isin(indices)]\nploting_cnt_amt(selected_rows, 'addr1')\n\n\n\n\n\n\n\nCode\nresult = df_trans.groupby(['addr2'])['addr2'].count().sort_values(ascending=False)\ntotal_count = result.sum()\nresult_percentage = (result / total_count) * 100\nresult_percentage\n\n\naddr2\n87.0    88.136451\nNAN     11.126427\n60.0     0.522234\n96.0     0.108037\n32.0     0.015410\n          ...    \n22.0     0.000169\n25.0     0.000169\n50.0     0.000169\n49.0     0.000169\n55.0     0.000169\nName: addr2, Length: 75, dtype: float64\n\n\nAlmost all addr2 is 87, which indicates that this may not be a meaningful variable.\n\n\nP_Email domain\n\nMost transactions come from Google, Yahoo and missing value.\nWhile Microsoft has the 4th highest transaction, it has the 1st fraud rate.\nWhile spectrum has relatively low fraud rate, it has the 1st percentage of fraud amount.\nes has higher fraud rate and amount.\n\n\n\nCode\nresumetable(df_trans, ['P_emaildomain'])\n\n\nDataset Shape: (590540, 399)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      P_emaildomain\n      object\n      94456\n      59\n      NaN\n      gmail.com\n      outlook.com\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_trans, 'P_emaildomain')\ndf_trans.P_emaildomain = df_trans.P_emaildomain.fillna('NAN')\n\n\nMissing value percentage for P_emaildomain: 15.99%\n\n\nLet’s first rename similar domains to a same group of domain.\n\n\nCode\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple', 'NAN':'NAN'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\nfor c in ['P_emaildomain']:\n    df_trans[c + '_bin'] = df_trans[c].map(emails)\n    \n    df_trans[c + '_suffix'] = df_trans[c].map(lambda x: str(x).split('.')[-1])\n    #If a suffix matches any of the values in the us_emails list, it's replaced with 'us' to indicate it's a U.S.-based domain. Otherwise, the original suffix remains.\n    df_trans[c + '_suffix'] = df_trans[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\n\n\n\nCode\nploting_cnt_amt(df_trans, 'P_emaildomain_bin')\n\n\n\n\n\n\nploting_cnt_amt(df_trans, 'P_emaildomain_suffix')\n\n\n\n\n\n\nR_Email domain\n\nOver 76% are missing value, so let’s exlcude this column from further analysis.\n\n\n\nCode\nresumetable(df_trans, ['R_emaildomain'])\n\n\nDataset Shape: (590540, 401)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      R_emaildomain\n      object\n      453249\n      60\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_trans, 'R_emaildomain')\n\n\nMissing value percentage for R_emaildomain: 76.75%\n\n\n\n\nC1-C14, D1-D9, M1-M9, Vxxx\nGiven that these features are masked and their meanings are not discernible from their values, to prioritize providing more interpretable insights, let’s exclude them from the subsequent analysis."
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-id",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-id",
    "title": "Fraud Detection",
    "section": "6-2. EDA-id",
    "text": "6-2. EDA-id\n\n\nCode\nresumetable(df_id, df_id.columns.tolist())\n\n\nDataset Shape: (144233, 41)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      TransactionID\n      int32\n      0\n      144233\n      2987004\n      2987008\n      2987010\n    \n    \n      1\n      id_01\n      float16\n      0\n      77\n      0.0\n      -5.0\n      -5.0\n    \n    \n      2\n      id_02\n      float32\n      3361\n      115655\n      70787.0\n      98945.0\n      191631.0\n    \n    \n      3\n      id_03\n      float16\n      77909\n      24\n      NaN\n      NaN\n      0.0\n    \n    \n      4\n      id_04\n      float16\n      77909\n      15\n      NaN\n      NaN\n      0.0\n    \n    \n      5\n      id_05\n      float16\n      7368\n      93\n      NaN\n      0.0\n      0.0\n    \n    \n      6\n      id_06\n      float16\n      7368\n      101\n      NaN\n      -5.0\n      0.0\n    \n    \n      7\n      id_07\n      float16\n      139078\n      84\n      NaN\n      NaN\n      NaN\n    \n    \n      8\n      id_08\n      float16\n      139078\n      94\n      NaN\n      NaN\n      NaN\n    \n    \n      9\n      id_09\n      float16\n      69307\n      46\n      NaN\n      NaN\n      0.0\n    \n    \n      10\n      id_10\n      float16\n      69307\n      62\n      NaN\n      NaN\n      0.0\n    \n    \n      11\n      id_11\n      float16\n      3255\n      146\n      100.0\n      100.0\n      100.0\n    \n    \n      12\n      id_12\n      object\n      0\n      2\n      NotFound\n      NotFound\n      NotFound\n    \n    \n      13\n      id_13\n      float16\n      16913\n      54\n      NaN\n      49.0\n      52.0\n    \n    \n      14\n      id_14\n      float16\n      64189\n      25\n      -480.0\n      -300.0\n      NaN\n    \n    \n      15\n      id_15\n      object\n      3248\n      3\n      New\n      New\n      Found\n    \n    \n      16\n      id_16\n      object\n      14893\n      2\n      NotFound\n      NotFound\n      Found\n    \n    \n      17\n      id_17\n      float16\n      4864\n      104\n      166.0\n      166.0\n      121.0\n    \n    \n      18\n      id_18\n      float16\n      99120\n      18\n      NaN\n      NaN\n      NaN\n    \n    \n      19\n      id_19\n      float16\n      4915\n      522\n      542.0\n      621.0\n      410.0\n    \n    \n      20\n      id_20\n      float16\n      4972\n      394\n      144.0\n      500.0\n      142.0\n    \n    \n      21\n      id_21\n      float16\n      139074\n      490\n      NaN\n      NaN\n      NaN\n    \n    \n      22\n      id_22\n      float16\n      139064\n      25\n      NaN\n      NaN\n      NaN\n    \n    \n      23\n      id_23\n      object\n      139064\n      3\n      NaN\n      NaN\n      NaN\n    \n    \n      24\n      id_24\n      float16\n      139486\n      12\n      NaN\n      NaN\n      NaN\n    \n    \n      25\n      id_25\n      float16\n      139101\n      341\n      NaN\n      NaN\n      NaN\n    \n    \n      26\n      id_26\n      float16\n      139070\n      95\n      NaN\n      NaN\n      NaN\n    \n    \n      27\n      id_27\n      object\n      139064\n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      28\n      id_28\n      object\n      3255\n      2\n      New\n      New\n      Found\n    \n    \n      29\n      id_29\n      object\n      3255\n      2\n      NotFound\n      NotFound\n      Found\n    \n    \n      30\n      id_30\n      object\n      66668\n      75\n      Android 7.0\n      iOS 11.1.2\n      NaN\n    \n    \n      31\n      id_31\n      object\n      3951\n      130\n      samsung browser 6.2\n      mobile safari 11.0\n      chrome 62.0\n    \n    \n      32\n      id_32\n      float16\n      66647\n      4\n      32.0\n      32.0\n      NaN\n    \n    \n      33\n      id_33\n      object\n      70944\n      260\n      2220x1080\n      1334x750\n      NaN\n    \n    \n      34\n      id_34\n      object\n      66428\n      4\n      match_status:2\n      match_status:1\n      NaN\n    \n    \n      35\n      id_35\n      object\n      3248\n      2\n      T\n      T\n      F\n    \n    \n      36\n      id_36\n      object\n      3248\n      2\n      F\n      F\n      F\n    \n    \n      37\n      id_37\n      object\n      3248\n      2\n      T\n      F\n      T\n    \n    \n      38\n      id_38\n      object\n      3248\n      2\n      T\n      T\n      T\n    \n    \n      39\n      DeviceType\n      object\n      3423\n      2\n      mobile\n      mobile\n      desktop\n    \n    \n      40\n      DeviceInfo\n      object\n      25567\n      1786\n      SAMSUNG SM-G892A Build/NRD90M\n      iOS Device\n      Windows\n    \n  \n\n\n\n\nFrom the table above, we can select some potentially useful variables including id_30, id_31, DeviceType, DeviceInfo.\n\n\nCode\n# merge two df for better analysis\ndf_merged = df_trans.merge(df_id, on='TransactionID', how='left')\n\n\n\nid_30\n\nThis column likely represents operation system.\nMore than 86% are missing values, so we’ll exclude this variable in the following analysis.\n\n\n\nCode\nresumetable(df_merged, ['id_30'])\n\n\nDataset Shape: (590540, 441)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      id_30\n      object\n      512975\n      75\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_merged, 'id_30')\n\n\nMissing value percentage for id_30: 86.87%\n\n\n\n\nid_31\n\nThis column likely represents browser type.\nMore than 70% are missing values, so we’ll exclude this variable in the following analysis.\n\n\n\nCode\nresumetable(df_merged, ['id_31'])\n\n\nDataset Shape: (590540, 441)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      id_31\n      object\n      450258\n      130\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_merged, 'id_31')\n\n\nMissing value percentage for id_31: 76.25%\n\n\n\n\nDeviceType\n\nThis column contains only two possibilites: desktop and mobile.\nMore than 70% are missing values, so we’ll exclude this variable in the following analysis.\n\n\n\nCode\nresumetable(df_merged, ['DeviceType'])\n\n\nDataset Shape: (590540, 441)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      DeviceType\n      object\n      449730\n      2\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\ncalculate_missing_percentage(df_merged, 'DeviceType')\n\n\nMissing value percentage for DeviceType: 76.16%\n\n\n\n\nCode\ndf_merged.DeviceType.value_counts()\n\n\ndesktop    85165\nmobile     55645\nName: DeviceType, dtype: int64\n\n\n\n\nDeviceInfo\n\nThis column also have around 80% missing values, so we’ll also exclude it.\n\n\n\nCode\nresumetable(df_merged, ['DeviceInfo'])\ncalculate_missing_percentage(df_merged, 'DeviceInfo')\ndf_merged.DeviceInfo.value_counts()\n\n\nDataset Shape: (590540, 441)\nMissing value percentage for DeviceInfo: 79.91%\n\n\nWindows        47722\niOS Device     19782\nMacOS          12573\nTrident/7.0     7440\nrv:11.0         1901\n               ...  \nLGMS345            1\nverykool           1\nXT1072             1\nLG-H931            1\n0PJA2              1\nName: DeviceInfo, Length: 1786, dtype: int64"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-summary",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#eda-summary",
    "title": "Fraud Detection",
    "section": "6-3. EDA Summary",
    "text": "6-3. EDA Summary\nAfter reviewing all columns, the following ones are selected for their meaningful content and will be used:\n\nDate\n_Weekdays\n_Hours\n_Days\nTransactionAmt\nLog_TransactionAmt\nProductCD\ncard4\ncard6\naddr1\nP_emaildomain_bin\nP_emaildomain_suffix"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#select-useful-columns",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#select-useful-columns",
    "title": "Fraud Detection",
    "section": "7-1. Select useful columns",
    "text": "7-1. Select useful columns\n\n\nCode\nselected_columns = [\n    'isFraud','Date', '_Weekdays', '_Hours', '_Days',\n    'TransactionAmt', 'Log_TransactionAmt', 'ProductCD', 'card4', 'card6',\n    'addr1', 'P_emaildomain_bin', 'P_emaildomain_suffix'\n]\n\n# Selecting the specified columns\ndf_train = df_merged[selected_columns]\n\n\n\n\nCode\nresumetable(df_train, df_train.columns.tolist())\n\n\nDataset Shape: (590540, 13)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      isFraud\n      int8\n      0\n      2\n      0\n      0\n      0\n    \n    \n      1\n      Date\n      datetime64[ns]\n      0\n      573349\n      2017-12-02 00:00:00\n      2017-12-02 00:00:01\n      2017-12-02 00:01:09\n    \n    \n      2\n      _Weekdays\n      int64\n      0\n      7\n      5\n      5\n      5\n    \n    \n      3\n      _Hours\n      int64\n      0\n      24\n      0\n      0\n      0\n    \n    \n      4\n      _Days\n      int64\n      0\n      31\n      2\n      2\n      2\n    \n    \n      5\n      TransactionAmt\n      float64\n      0\n      8195\n      68.5\n      29.0\n      59.0\n    \n    \n      6\n      Log_TransactionAmt\n      float64\n      0\n      8195\n      4.241327\n      3.401197\n      4.094345\n    \n    \n      7\n      ProductCD\n      object\n      0\n      5\n      W\n      W\n      W\n    \n    \n      8\n      card4\n      object\n      0\n      5\n      discover\n      mastercard\n      visa\n    \n    \n      9\n      card6\n      object\n      0\n      5\n      credit\n      credit\n      debit\n    \n    \n      10\n      addr1\n      object\n      0\n      333\n      315.0\n      325.0\n      330.0\n    \n    \n      11\n      P_emaildomain_bin\n      object\n      0\n      10\n      NAN\n      google\n      microsoft\n    \n    \n      12\n      P_emaildomain_suffix\n      object\n      0\n      9\n      NAN\n      com\n      com"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#logistic-regression",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#logistic-regression",
    "title": "Fraud Detection",
    "section": "7-2. Logistic Regression",
    "text": "7-2. Logistic Regression\nLet’s start with a logistic regression as our base line model.\n\nChange data type from int to object\n\n_Weekdays\n_Hours\n_Days\n\nGroup low frequency values of addr1:\n\nAvoid overfitting and curse of dimensionality\n\nSet Date to index and sort ascending:\n\nfor time series data, it’s better to split by time to make sure training data is older than test data, which is different from normal train test split.\n\nOne hot encoding:\n\n_Weekdays\n_Hours\n_Days\nProductCD\ncard4\ncard6\naddr1\nP_emaildomain_bin\nP_emaildomain_suffix\n\nStanderdization:\n\nTransactionAmt\n\nSplit data\nHow to deal with imbalance dataset? (oversampling techinque discussion):\n\nRecall that in EDA of isFraud variable that fraud transactions occur far less frequently compared to non-fraudulent ones. To address this class imbalance, performing oversampling specifically on fraud transactions becomes crucial, allowing us to better capture and train the model on these rare instances.\n\nOverfitting:\n\nUsing regularization can avoid overfitting problem, but regularization performed on logistic regression with this dataset is not very prominant. We’ll further explore other models that may solve this problem.\n\nConclusion:\n\nThe best model’s precision-recall auc is 0.37, which can can capture 63 % of fraud transactions and the prediction precision is 9 %. 24 % of non-fraud transactions are misclassified\n\n\n1.change data type from int to object\n\n\nCode\n# 1. change data type from int to object\ndf_train[['_Weekdays', '_Hours', '_Days']] = df_train[['_Weekdays', '_Hours', '_Days']].astype('object')\nresumetable(df_train, ['_Weekdays', '_Hours', '_Days'])\n\n\nDataset Shape: (590540, 13)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      _Weekdays\n      object\n      0\n      7\n      5\n      5\n      5\n    \n    \n      1\n      _Hours\n      object\n      0\n      24\n      0\n      0\n      0\n    \n    \n      2\n      _Days\n      object\n      0\n      31\n      2\n      2\n      2\n    \n  \n\n\n\n\n2.Group low frequency values of addr1\n\n\nCode\n# 2. Group low frequency (<1%)values of addr1\nresult = df_train.groupby('addr1').size().sort_values(ascending=False)\ntotal_count = result.sum()\nresult_percentage = (result / total_count) * 100\n# Identify values with percentages less than 1\nvalues_to_group = result_percentage[result_percentage < 1].index.tolist()\n# Replace those values in the DataFrame with the new category 'others'\ndf_train['addr1'] = df_train['addr1'].apply(lambda x: 'others' if x in values_to_group else x)\nprint('reduce unique values of addr1 from 333 to 27')\nresumetable(df_train, ['addr1'])\n\n\nreduce unique values of addr1 from 333 to 27\nDataset Shape: (590540, 13)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      dtypes\n      Missing\n      Uniques\n      First Value\n      Second Value\n      Third Value\n    \n  \n  \n    \n      0\n      addr1\n      object\n      0\n      27\n      315.0\n      325.0\n      330.0\n    \n  \n\n\n\n\n3.set Date to index and sort ascending\n\n\nCode\n# 3.set Date to index and sort ascending\ndf_train.set_index('Date', inplace=True)\ndf_train.sort_index(ascending=True, inplace=True)\n\n\n4.One hot encoding\n\n\nCode\n# 4.One hot encoding\n# Splitting into features and target variable\nX = df_train.drop('isFraud', axis=1)\ny = df_train['isFraud']\n\n# List of categorical columns for one hot encoding\ncolumns_to_encode = ['_Weekdays', '_Hours', '_Days', 'ProductCD', 'card4', 'card6','addr1', 'P_emaildomain_bin', 'P_emaildomain_suffix']\nX_transformed = pd.get_dummies(X, columns=columns_to_encode, prefix=columns_to_encode)\n\n\n5.Standerdization\n\n\nCode\n#5. Standerdization:\ncolumns_to_standerdize = ['TransactionAmt']\n# Initialize the StandardScaler\nscaler = StandardScaler()\n# Fit and transform the scaler on your numeric data\nX_scaled = scaler.fit_transform(X[columns_to_standerdize])\n# replcae with original TransactionAmt\nX_transformed['TransactionAmt'] = X_scaled\n\n\n\nX_transformed.head(3)\n\n\n\n\n\n  \n    \n      \n      TransactionAmt\n      Log_TransactionAmt\n      _Weekdays_0\n      _Weekdays_1\n      _Weekdays_2\n      _Weekdays_3\n      _Weekdays_4\n      _Weekdays_5\n      _Weekdays_6\n      _Hours_0\n      ...\n      P_emaildomain_bin_yahoo\n      P_emaildomain_suffix_NAN\n      P_emaildomain_suffix_com\n      P_emaildomain_suffix_de\n      P_emaildomain_suffix_es\n      P_emaildomain_suffix_fr\n      P_emaildomain_suffix_jp\n      P_emaildomain_suffix_mx\n      P_emaildomain_suffix_uk\n      P_emaildomain_suffix_us\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017-12-02 00:00:00\n      -0.278174\n      4.241327\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2017-12-02 00:00:01\n      -0.443337\n      3.401197\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2017-12-02 00:01:09\n      -0.317897\n      4.094345\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n3 rows × 125 columns\n\n\n\n6.Split data\n\n\nCode\n#6. Split data\n# Check the range of dates\nmin_date = X_transformed.index.min()\nmax_date = X_transformed.index.max()\n\nprint(f\"Minimum date: {min_date}\")\nprint(f\"Maximum date: {max_date}\")\n\n\nMinimum date: 2017-12-02 00:00:00\nMaximum date: 2018-06-01 23:58:51\n\n\nThe date range spans approximately 6 months. Given that it’s a time series dataset, it’s more appropriate to partition it based on dates rather than employing the conventional train-test split. Let’s use the first 5 months as training data and the last month as testing data.\n\ntrain_date = '2018-04-30'\ntest_date = '2018-05-01'\nx_train, y_train = X_transformed.loc[:train_date], y.loc[:train_date]\nx_test, y_test = X_transformed.loc[test_date:], y.loc[test_date:]\n\n7.How to deal with imbalance dataset? (oversampling techinque discussion)\nRecall that in our EDA “isFraud” section, it was observed that only 3.5% of records were labeled as ‘fraud.’ This class imbalance poses a challenge for model performance, as the model may struggle to capture the characteristics of fraudulent records effectively. Consequently, employing oversampling techniques becomes crucial to augment the number of fraud transactions, enhancing the model’s ability for more effective modeling.\nLet’s start by building the model without oversampling.\n\n\nCode\n# 8. Perform logistic regression\nlogreg = LogisticRegression(random_state=42)\n\n# Train the model using the training data\nlogreg.fit(x_train, y_train)\n\n# Predict on the train data\nx_train_pred = logreg.predict(x_train)\n# Predict on the test data\ny_pred = logreg.predict(x_test)\n\n\n\n\nCode\n#show performance\nprint(\"(Without Oversampling): Show performance on training data...\")\nevaluate_results(y_train, x_train_pred)\n\n\n(Without Oversampling): Show performance on training data...\nF1 Score: 0.005022257733135487\nConfusion Matrix:\n[[480635     28]\n [ 17406     44]]\nPrecision is 61 %; Recall is 0 %\nPR-RC AUC: 0.32428823963938536\n\n\n\n\n\nCan capture 0 % of fraud transactions and the prediction precision is 61 %. 0 % of non-fraud transactions are misclassified\n\n\n\n\nCode\n#show performance\nprint(\"(Without Oversampling): Show performance on testing data...\")\nevaluate_results(y_test, y_pred)\n\n\n(Without Oversampling): Show performance on testing data...\nF1 Score: 0.0037243947858473002\nConfusion Matrix:\n[[89211     3]\n [ 3207     6]]\nPrecision is 67 %; Recall is 0 %\nPR-RC AUC: 0.35161586679095885\n\n\n\n\n\nCan capture 0 % of fraud transactions and the prediction precision is 67 %. 0 % of non-fraud transactions are misclassified\n\n\nWithout oversampling PR-RC-AUC: - Training data: 0.32 - Testing data: 0.35\nNow let’s perform oversampling and observe the changes.\n\n\nCode\n#7. Perform oversampling to solve imbalance problem\n# https://www.kaggle.com/code/shahules/tackling-class-imbalance\nX=pd.concat([x_train,y_train],axis=1)\n\n\nnot_fraud=X[X.isFraud==0]\nfraud=X[X.isFraud==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# replace original x_train, y_train\nx_train = upsampled.drop('isFraud', axis=1)\ny_train = upsampled['isFraud']\n\n# check new class counts\nprint(\"check new class counts\")\nupsampled.isFraud.value_counts()\n\n\ncheck new class counts\n\n\n0    480663\n1    480663\nName: isFraud, dtype: int64\n\n\n\n\nCode\n# 8. Perform logistic regression\nlogreg = LogisticRegression(random_state=42)\n\n# Train the model using the training data\nlogreg.fit(x_train, y_train)\n\n# Predict on the train data\nx_train_pred = logreg.predict(x_train)\n# Predict on the test data\ny_pred = logreg.predict(x_test)\n\n\n\n\nCode\n#show performance\nprint(\"(With Oversampling): Show performance on training data...\")\nevaluate_results(y_train, x_train_pred)\n\n\n(With Oversampling): Show performance on training data...\nF1 Score: 0.6837879685114658\nConfusion Matrix:\n[[355648 125015]\n [166006 314657]]\nPrecision is 72 %; Recall is 65 %\nPR-RC AUC: 0.7714893216881513\n\n\n\n\n\nCan capture 65 % of fraud transactions and the prediction precision is 72 %. 26 % of non-fraud transactions are misclassified\n\n\n\n\nCode\n#show performance\nprint(\"(With Oversampling): Show performance on testing data...\")\nevaluate_results(y_test, y_pred)\n\n\n(With Oversampling): Show performance on testing data...\nF1 Score: 0.15101787215379556\nConfusion Matrix:\n[[67132 22082]\n [ 1147  2066]]\nPrecision is 9 %; Recall is 64 %\nPR-RC AUC: 0.37048914697029617\n\n\n\n\n\nCan capture 64 % of fraud transactions and the prediction precision is 9 %. 25 % of non-fraud transactions are misclassified\n\n\nWith oversampling PR-RC-AUC: - Training data: 0.77 - Testing data: 0.37\nWith oversampling, training data’s PR-RC-AUC increases from 0.32 to 0.77 and testing data’s PR-RC-AUC increases from 0.35 to 0.37. This shows the oversampling technique help the model to better capture characteristics of fraudulent transactions.\n\nSolve overfitting\n\nThe Precision-Recall Area Under the Curve (PR-RC-AUC) for the testing data is lower at 0.37 compared to the training data’s higher PR-RC-AUC of 0.77. This discrepancy suggests the presence of an overfitting problem. To address this issue, let’s implement some regularization techniques.\n\n#Standardization is often recommended when using regularization.\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\n\n\nCode\n# 8. Perform logistic regression\nlogreg = LogisticRegression(random_state=42, C=0.00001)\n\n# Train the model using the training data\nlogreg.fit(x_train_scaled, y_train)\n\n# Predict on the train data\nx_train_pred = logreg.predict(x_train_scaled)\n# Predict on the test data\ny_pred = logreg.predict(x_test_scaled)\n\n\n\n\nCode\n#show performance\nevaluate_results(y_train, x_train_pred)\n\n\nF1 Score: 0.6758616444559736\nConfusion Matrix:\n[[358994 121669]\n [173223 307440]]\nPrecision is 72 %; Recall is 64 %\nPR-RC AUC: 0.768134791760063\n\n\n\n\n\nCan capture 64 % of fraud transactions and the prediction precision is 72 %. 25 % of non-fraud transactions are misclassified\n\n\n\n\nCode\n#show performance\nevaluate_results(y_test, y_pred)\n\n\nF1 Score: 0.1540732889158086\nConfusion Matrix:\n[[68046 21168]\n [ 1178  2035]]\nPrecision is 9 %; Recall is 63 %\nPR-RC AUC: 0.3669069132910227\n\n\n\n\n\nCan capture 63 % of fraud transactions and the prediction precision is 9 %. 24 % of non-fraud transactions are misclassified\n\n\nRegularization performed on logistic regression is not very prominant. Let’s further explore other models that may solve this problem."
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#decision-tree",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#decision-tree",
    "title": "Fraud Detection",
    "section": "7-3. Decision Tree",
    "text": "7-3. Decision Tree\n\nChange data type from int to object\n\n_Weekdays\n_Hours\n_Days\n\nGroup low frequency values of addr1:\n\nAvoid overfitting and curse of dimensionality\n\nSet Date to index and sort ascending:\n\nfor time series data, it’s better to split by time to make sure training data is older than test data, which is different from normal train test split.\n\nOne hot encoding: Decision Tree is able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now.\n\n_Weekdays\n_Hours\n_Days\nProductCD\ncard4\ncard6\naddr1\nP_emaildomain_bin\nP_emaildomain_suffix\n\nStanderdization:\n\nTransactionAmt\n\nSplit data\nPerform oversampling to solve imbalance problem:\n\nRecall that in EDA of isFraud variable that fraud transactions occur far less frequently compared to non-fraudulent ones. To address this class imbalance, performing oversampling specifically on fraud transactions becomes crucial, allowing us to better capture and train the model on these rare instances.\n\nPerform deicision tree\nEvaluate Results:\n\nConfusion Matrix\nF1-score\nRecall-Precision curve AUC\n\nConclusion:\n\nThe best model’s precision-recall auc is 0.36, which can capture 62 % of fraud transactions and the prediction precision is 8 %. 26 % of non-fraud transactions are misclassified\n\n\n\n\nCode\n# Selecting the specified columns\ndf_train = df_merged[selected_columns]\n# 1. change data type from int to object\ndf_train[['_Weekdays', '_Hours', '_Days']] = df_train[['_Weekdays', '_Hours', '_Days']].astype('object')\n\n# 2. Group low frequency values of addr1\ndf_train = group_low_percentage_values(df_train, 'addr1', threshold=1)\n\n# 3. Set Date to index and sort ascending\ndf_train.set_index('Date', inplace=True)\ndf_train.sort_index(ascending=True, inplace=True)\n\n# Splitting into features and target variable\nX = df_train.drop('isFraud', axis=1)\ny = df_train['isFraud']\n\n# 4. One hot encoding\ncolumns_to_encode = ['_Weekdays', '_Hours', '_Days', 'ProductCD', 'card4', 'card6','addr1', 'P_emaildomain_bin', 'P_emaildomain_suffix']\nX_transformed = pd.get_dummies(X, columns=columns_to_encode, prefix=columns_to_encode)\n\n# 5. Standerdization\ncolumns_to_standerdize = ['TransactionAmt']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X[columns_to_standerdize])\nX_transformed['TransactionAmt'] = X_scaled\n\n# 6. Split data\ntrain_date = '2018-04-30'\ntest_date = '2018-05-01'\nx_train, y_train, x_test, y_test = split_data_by_date(X_transformed, y, train_date, test_date)\n\n# 7. oversampling\nX=pd.concat([x_train,y_train],axis=1)\nupsampled_data = upsample_minority_class(X)\nx_train = upsampled.drop('isFraud', axis=1)\ny_train = upsampled['isFraud']\n\n\n\n\nCode\n#8. Decision Tree\n\n# 1% of total values\nthreshold = round(x_train.shape[0]*0.001)\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(max_depth=20, min_samples_split=threshold, min_samples_leaf = threshold, random_state=42)\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n\n# Predict on the train data\nx_train_pred = clf.predict(x_train)\n# Predict on the test data\ny_pred = clf.predict(x_test)\n\n\n\n\nCode\nevaluate_results(y_train, x_train_pred)\n\n\nF1 Score: 0.7293538142181031\nConfusion Matrix:\n[[368797 111866]\n [140550 340113]]\nPrecision is 75 %; Recall is 71 %\nPR-RC AUC: 0.8031465226186614\n\n\n\n\n\nCan capture 71 % of fraud transactions and the prediction precision is 75 %. 23 % of non-fraud transactions are misclassified\n\n\n\n\nCode\nevaluate_results(y_test, y_pred)\n\n\nF1 Score: 0.15381615381615382\nConfusion Matrix:\n[[68758 20456]\n [ 1241  1972]]\nPrecision is 9 %; Recall is 61 %\nPR-RC AUC: 0.35755461664752\n\n\n\n\n\nCan capture 61 % of fraud transactions and the prediction precision is 9 %. 23 % of non-fraud transactions are misclassified"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#xgboost",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#xgboost",
    "title": "Fraud Detection",
    "section": "7-4. XGBoost",
    "text": "7-4. XGBoost\n\nChange data type from int to object\n\n_Weekdays\n_Hours\n_Days\n\nGroup low frequency values of addr1:\n\nAvoid overfitting and curse of dimensionality\n\nSet Date to index and sort ascending:\n\nfor time series data, it’s better to split by time to make sure training data is older than test data, which is different from normal train test split.\n\nOne hot encoding:\n\n_Weekdays\n_Hours\n_Days\nProductCD\ncard4\ncard6\naddr1\nP_emaildomain_bin\nP_emaildomain_suffix\n\nStanderdization:\n\nTransactionAmt\n\nSplit data\nPerform oversampling to solve imbalance problem:\n\nRecall that in EDA of isFraud variable that fraud transactions occur far less frequently compared to non-fraudulent ones. To address this class imbalance, performing oversampling specifically on fraud transactions becomes crucial, allowing us to better capture and train the model on these rare instances.\n\nPerform XGBoost\nEvaluate Results:\n\nConfusion Matrix\nF1-score\nRecall-Precision curve AUC\n\nConclusion:\n\nThe best model’s precision-recall auc is 0.36, which can capture 59 % of fraud transactions and the prediction precision is 11 %. 17 % of non-fraud transactions are misclassified.\n\n\n\n\nCode\n# Selecting the specified columns\ndf_train = df_merged[selected_columns]\n# 1. change data type from int to object\ndf_train[['_Weekdays', '_Hours', '_Days']] = df_train[['_Weekdays', '_Hours', '_Days']].astype('object')\n\n# 2. Group low frequency values of addr1\ndf_train = group_low_percentage_values(df_train, 'addr1', threshold=1)\n\n# 3. Set Date to index and sort ascending\ndf_train.set_index('Date', inplace=True)\ndf_train.sort_index(ascending=True, inplace=True)\n\n# Splitting into features and target variable\nX = df_train.drop('isFraud', axis=1)\ny = df_train['isFraud']\n\n# 4. One hot encoding\ncolumns_to_encode = ['_Weekdays', '_Hours', '_Days', 'ProductCD', 'card4', 'card6','addr1', 'P_emaildomain_bin', 'P_emaildomain_suffix']\nX_transformed = pd.get_dummies(X, columns=columns_to_encode, prefix=columns_to_encode)\n\n# 5. Standerdization\ncolumns_to_standerdize = ['TransactionAmt']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X[columns_to_standerdize])\nX_transformed['TransactionAmt'] = X_scaled\n\n# 6. Split data\ntrain_date = '2018-04-30'\ntest_date = '2018-05-01'\nx_train, y_train, x_test, y_test = split_data_by_date(X_transformed, y, train_date, test_date)\n\n# 7. oversampling\nX=pd.concat([x_train,y_train],axis=1)\nupsampled_data = upsample_minority_class(X)\nx_train = upsampled.drop('isFraud', axis=1)\ny_train = upsampled['isFraud']\n\n\n\n\nCode\n#8. Decision Tree\n# Create XGBClassifier\nclf = xgb.XGBClassifier(\n            n_estimators=100, random_state=4, verbose=True, \n            tree_method='gpu_hist',objective='binary:logistic', max_depth=10, learning_rate=0.02, \n            subsample=0.8, colsample_bytree=0.4, eval_metric='aucpr'\n        )\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n\n#Predict the response for test dataset\nx_train_pred = clf.predict(x_train)\n#Predict the response for test dataset\ny_pred = clf.predict(x_test)\n\n\n\n\nCode\nevaluate_results(y_train, x_train_pred)\n\n\nF1 Score: 0.7630705589039241\nConfusion Matrix:\n[[394996  85667]\n [131290 349373]]\nPrecision is 80 %; Recall is 73 %\nPR-RC AUC: 0.8332553491374364\n\n\n\n\n\nCan capture 73 % of fraud transactions and the prediction precision is 80 %. 18 % of non-fraud transactions are misclassified\n\n\n\n\nCode\nevaluate_results(y_test, y_pred)\n\n\nF1 Score: 0.1855419342284491\nConfusion Matrix:\n[[73778 15436]\n [ 1306  1907]]\nPrecision is 11 %; Recall is 59 %\nPR-RC AUC: 0.35880713885797433\n\n\n\n\n\nCan capture 59 % of fraud transactions and the prediction precision is 11 %. 17 % of non-fraud transactions are misclassified"
  },
  {
    "objectID": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#conclusion",
    "href": "Data_Analytics_Projects/Fraud Detection/Fraud_Detection.html#conclusion",
    "title": "Fraud Detection",
    "section": "7-5. Conclusion",
    "text": "7-5. Conclusion\nUntil now, we have experimented with three models (logistic regression, decision tree, xgboost); however, there has been no significant improvement in results. I suspect that the lack of improvement may not be due to the models’ capabilities but rather a result of not utilizing the most informative features. The excluded features, such as C, D, M, and V, may contain some useful information, but their true meaning were not provided which give no meaningful insights. Therefore, we will not delve deeper into analyzing these features."
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html",
    "title": "Demographic Visual Analysis",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received."
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#participants-dataset",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#participants-dataset",
    "title": "Demographic Visual Analysis",
    "section": "5.1 Participants Dataset",
    "text": "5.1 Participants Dataset\nThe spec() function will be used because newer versions of readr don’t report the full column specification when loading data files. This function will help us better understand the complete column specification.\nTo correct problematic columns, we will utilize dplyr::mutate to make necessary corrections:\n\nparticipantId is in <dbl> format. It should be reformatted to <factor>.\ninterestGroup is in <chr> format. It should be reformatted to <factor>.\neducationLevel is in <chr> format. It should be reformatted to factor, and ordered from low to high.\n\n\n\nCode\nspec(participants)\n\nparticipants <- participants %>% mutate_at(c('participantId', 'interestGroup', 'educationLevel'), as.factor)\n\nparticipants$educationLevel <- ordered(participants$educationLevel, levels = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\"))\n\n\nIn order to have the flexibility to analyse the age variable in bands, we will also re-code the age variable into 10 year bands using the cut() function. The new variable is saved under age_band.\n\n\nCode\n# Recode ages into 10-year age bands\n# Define breaks and labels\nbreaks <- seq(10, 70, by = 10)\nlabels <- c(\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60+\")\n\n# Recode age variable\nparticipants$age_band <- cut(participants$age, breaks = breaks, labels = labels)"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#financialjournal-dataset",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#financialjournal-dataset",
    "title": "Demographic Visual Analysis",
    "section": "5.2 FinancialJournal Dataset",
    "text": "5.2 FinancialJournal Dataset\nMuch like the Participants dataset, several problems were noted within the FinancialJournal dataset too. These included:\n\nparticipantId is in <dbl> format. It should be reformatted to <factor>.\ncategory is in <chr> format. It should be reformatted to <factor>.\n\n\n\nCode\nspec(finance)\nfinance <-finance %>% mutate_at(c('participantId', 'category'), as.factor)\n\n\nCurrently, the variable timestamp provides very micro level breakdown of spending pattern; down to the minute. This may not be useful when trying to understanding broad consumption patterns. As such, using lubridate::as.Date the timestamp variable was reformatted to “%Y-%m-%d” and a new variable, MonthYear was created to extract only the year and month data from timestamp.\n\n\nCode\nfinance$timestamp <- as.Date(finance$timestamp, format = \"%Y-%m-%d\")\n\n#Extracting month year\nfinance$MonthYear <- format(as.Date(finance$timestamp), \"%Y-%m\")\n\n\nThe Finance dataset also revealed 1,113 duplicated rows. To ensure accuracy in subsequent analyses, these rows will be eliminated using distinct(). The resulting trimmed dataset will be stored as fin_new.\n\n\nCode\nfin_new <- finance %>% distinct()\n\n\nThe amount variable in the FinancialJournal records money inflow and outflow as positive and negative numbers, respectively. This setup might lead to confusion while visualizing the data. To address this, we’ll process the data by taking the absolute values using abs().\n\n\nCode\nfin_new$amount <- abs(fin_new$amount)\n\n\nTo ensure the data’s completeness, we’re examining it at the participant level. The histogram displays a distinct cluster showing 131 participants with very few, nearly zero transactions. This could be due to participants dropping out of the study or relocating away from town. Considering the lack of financial data collected from these participants, they will be excluded from further analyses using the filter() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nfin_grouped <- fin_new %>%\n  group_by(participantId) %>%\n  dplyr::summarize(transaction_count = n()) %>%\n  arrange(transaction_count)\n\np <- ggplot(data=fin_grouped, \n       aes(x = transaction_count)) +\n  geom_histogram(bins = 39,\n                color=\"black\",      \n                fill=\"steelblue\") +\n  ggtitle(\"Distribution of Transactions among Participants\") +\n  xlab(\"Transaction Count\") + \n  ylab(\"Number of Participants\") +\n  theme_classic()\n\np <- ggplotly(p, tooltip=c(\"y\"))\n\n# display the plot\np\n\n\n\n\n\n\nCode\n# Find participant IDs in fin_grouped with transaction count < 500\nparticipants_to_remove <- fin_grouped %>%\n  filter(transaction_count < 500) %>%\n  pull(participantId)\n\n# Filter out rows in fin_new for those participants\nfin_new <- fin_new %>%\n  filter(!participantId %in% participants_to_remove)\n\n\nCurrently, the data is in a long format, with each row representing 1 transaction. We will transform the data using pivot_wider() to a wide format instead.\n\nfin_final <- fin_new %>%\n  group_by(participantId, category, MonthYear) %>%\n  summarise(Total = sum(amount))\n\nfin_final\n\n# A tibble: 21,627 × 4\n# Groups:   participantId, category [3,702]\n   participantId category  MonthYear Total\n   <fct>         <fct>     <chr>     <dbl>\n 1 0             Education 2022-03    38.0\n 2 0             Education 2022-04    38.0\n 3 0             Education 2022-05    38.0\n 4 0             Education 2022-06    38.0\n 5 0             Education 2022-07    38.0\n 6 0             Education 2022-08    38.0\n 7 0             Food      2022-03   253. \n 8 0             Food      2022-04   238. \n 9 0             Food      2022-05   251. \n10 0             Food      2022-06   247. \n# ℹ 21,617 more rows\n\n\n\nfin_final <- pivot_wider(\n  fin_final,names_from = category,values_from =Total)\nfin_final\n\n# A tibble: 5,118 × 8\n# Groups:   participantId [853]\n   participantId MonthYear Education  Food Recreation Shelter   Wage\n   <fct>         <chr>         <dbl> <dbl>      <dbl>   <dbl>  <dbl>\n 1 0             2022-03        38.0  253.       345.    555. 11932.\n 2 0             2022-04        38.0  238.       219.    555.  8637.\n 3 0             2022-05        38.0  251.       383.    555.  9048.\n 4 0             2022-06        38.0  247.       444.    555.  9048.\n 5 0             2022-07        38.0  255.      1047.    555.  8637.\n 6 0             2022-08        38.0  250.       295.    555.  9459.\n 7 1             2022-03        38.0  280.      1031.    555. 10359.\n 8 1             2022-04        38.0  208.       504.    555.  7590.\n 9 1             2022-05        38.0  165.       359.    555.  7951.\n10 1             2022-06        38.0  228.       675.    555.  7951.\n# ℹ 5,108 more rows\n# ℹ 1 more variable: RentAdjustment <dbl>\n\n\n\nfin_wide <- fin_final %>%\n  pivot_wider(names_from = MonthYear, values_from = c(Education, Food, Recreation, Shelter, Wage, RentAdjustment))\nfin_wide\n\n# A tibble: 853 × 37\n# Groups:   participantId [853]\n   participantId `Education_2022-03` `Education_2022-04` `Education_2022-05`\n   <fct>                       <dbl>               <dbl>               <dbl>\n 1 0                            38.0                38.0                38.0\n 2 1                            38.0                38.0                38.0\n 3 2                            12.8                12.8                12.8\n 4 3                            38.0                38.0                38.0\n 5 4                            12.8                12.8                12.8\n 6 5                            12.8                12.8                12.8\n 7 6                            12.8                12.8                12.8\n 8 7                            12.8                12.8                12.8\n 9 8                            12.8                12.8                12.8\n10 9                            91.1                91.1                91.1\n# ℹ 843 more rows\n# ℹ 33 more variables: `Education_2022-06` <dbl>, `Education_2022-07` <dbl>,\n#   `Education_2022-08` <dbl>, `Food_2022-03` <dbl>, `Food_2022-04` <dbl>,\n#   `Food_2022-05` <dbl>, `Food_2022-06` <dbl>, `Food_2022-07` <dbl>,\n#   `Food_2022-08` <dbl>, `Recreation_2022-03` <dbl>,\n#   `Recreation_2022-04` <dbl>, `Recreation_2022-05` <dbl>,\n#   `Recreation_2022-06` <dbl>, `Recreation_2022-07` <dbl>, …\n\n\n\n#convert all NA values to 0 \nfin_wide[is.na(fin_wide)] <- 0\n\nBefore merging, we will create the following new variables using mutate():\n\nTotal Expenditure across categories, across months e.g., TotExp_Mar\nTotal Earnings, across months e.g., TotEarn_Mar\nTotal Savings, across months calculated using TotEarn - TotExp for each month\n\n\n\nCode\n#Total expenditure across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotExp_Mar = `Education_2022-03` + `Food_2022-03` + `Recreation_2022-03` + `Shelter_2022-03`,\n    TotExp_Apr = `Education_2022-04` + `Food_2022-04` + `Recreation_2022-04` + `Shelter_2022-04`,\n    TotExp_May = `Education_2022-05` + `Food_2022-05` + `Recreation_2022-05` + `Shelter_2022-05`,\n    TotExp_Jun = `Education_2022-06` + `Food_2022-06` + `Recreation_2022-06` + `Shelter_2022-06`,\n    TotExp_Jul = `Education_2022-07` + `Food_2022-07` + `Recreation_2022-07` + `Shelter_2022-07`,\n    TotExp_Aug = `Education_2022-08` + `Food_2022-08` + `Recreation_2022-08` + `Shelter_2022-08`\n)\n\n#Total Earnings across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotEarn_Mar = `Wage_2022-03` + ifelse(is.na(`RentAdjustment_2022-03`), 0, `RentAdjustment_2022-03`),\n    TotEarn_Apr = `Wage_2022-04` + ifelse(is.na(`RentAdjustment_2022-04`), 0, `RentAdjustment_2022-04`),\n    TotEarn_May = `Wage_2022-05` + ifelse(is.na(`RentAdjustment_2022-05`), 0, `RentAdjustment_2022-05`),\n    TotEarn_Jun = `Wage_2022-06` + ifelse(is.na(`RentAdjustment_2022-06`), 0, `RentAdjustment_2022-06`),\n    TotEarn_Jul = `Wage_2022-07` + ifelse(is.na(`RentAdjustment_2022-07`), 0, `RentAdjustment_2022-07`),\n    TotEarn_Aug = `Wage_2022-08` + ifelse(is.na(`RentAdjustment_2022-08`), 0, `RentAdjustment_2022-08`))\n\n#Total Savings across months\nfin_wide <- fin_wide %>%\n  mutate(\n    TotSav_Mar = TotEarn_Mar - TotExp_Mar,\n    TotSav_Apr = TotEarn_Apr - TotExp_Apr,\n    TotSav_May = TotEarn_May - TotExp_May,\n    TotSav_Jun = TotEarn_Jun - TotExp_Jun,\n    TotSav_Jul = TotEarn_Jul - TotExp_Jul,\n    TotSav_Aug = TotEarn_Aug - TotExp_Aug\n  )"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#combining-both-datasets",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#combining-both-datasets",
    "title": "Demographic Visual Analysis",
    "section": "5.3 Combining both Datasets",
    "text": "5.3 Combining both Datasets\nFinally, utilizing inner_join(), the datasets (participants and fin_wide) will be merged, enabling comparisons of financial data across various demographic groups. The resulting dataset will be stored as final_data.\n\n\nCode\n# inner join the datasets\n# merge the datasets\nmerged <- merge(fin_wide, participants, by = \"participantId\", all.x = TRUE)\n\n# subset the merged dataset to keep only the rows with participantId in fin_wide\nfinal_data <- subset(merged, participantId %in% fin_wide$participantId)"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#household-size-and-kids",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#household-size-and-kids",
    "title": "Demographic Visual Analysis",
    "section": "6.1 Household Size and Kids",
    "text": "6.1 Household Size and Kids\nResidents of the city tended to have small families; no more than 3 members per household.\nAll families with 3 members have kids.\n\n\nCode\n# Calculate the proportion of respondents in each HHsize band category\nfinal_data$householdSize <- as.factor(final_data$householdSize)\nhousehold_props <- final_data %>%\n  group_by(householdSize) %>%\n  summarize(count = n()) %>%\n  mutate(prop = count / sum(count))\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np3 <- ggplot(household_props, aes(x = \"\", y = prop, fill = householdSize)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"pink1\", \"pink3\", \"pink4\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Household Size\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Household Size(%)\")\np3\n\n\n\n\n\n# Calculate the percentage of households with kids for each household size\nhouseholdsize_kid_props <- final_data %>%\n  group_by(householdSize,haveKids) %>%\n  summarize(count = n()) %>%\n  mutate(prop = count / sum(count))\nhouseholdsize_kid_props\n\n# A tibble: 3 × 4\n# Groups:   householdSize [3]\n  householdSize haveKids count  prop\n  <fct>         <lgl>    <int> <dbl>\n1 1             FALSE      318     1\n2 2             FALSE      292     1\n3 3             TRUE       243     1"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#age-and-education",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#age-and-education",
    "title": "Demographic Visual Analysis",
    "section": "6.2 Age and Education",
    "text": "6.2 Age and Education\nThe age of residents living in the city were relatively evenly distributed, with close to one-third of the population (30%) made up by younger respondents below the ages of 30. Close to half were high school or college educated, while a similar proportion had a bachelor’s degree or higher.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate the proportion of respondents in each age band category\nage_band_props <- final_data %>%\n  group_by(age_band) %>%\n  summarize(count = n()) %>%\n  mutate(prop = count / sum(count))\n\n# Create a pie chart with the age band proportions\np1 <- ggplot(age_band_props, aes(x = \"\", y = prop, fill = age_band)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"skyblue1\", \"skyblue2\", \"skyblue3\", \"skyblue4\", \"darkslategrey\", \"grey\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Age Band\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Age Band (%)\")\n\n# Calculate the proportion of respondents in each education band category\neducation_props  <- final_data %>%\n  group_by(educationLevel) %>%\n  summarize(count = n()) %>%\n  mutate(prop = count / sum(count))\n\n# Create a pie chart with the age band proportions\np2 <- ggplot(education_props, aes(x = \"\", y = prop, fill = educationLevel)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0, direction = -1) +\n  scale_fill_manual(values = c(\"darkseagreen1\", \"darkseagreen2\", \"darkseagreen3\", \"darkseagreen4\")) +\n  theme_void() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 8)) +\n  labs(fill = \"Education Level\") +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  ggtitle(\"Residents by Education Level(%)\")\n\n(p1 + p2)\n\n\n\n\nMoving forward, let’s investigate the potential correlation between age and education. Given the departure of age distribution from normality, we’ve opted for the non-parametric Kruskal-Wallis test. After examining the boxplots, it became evident that there isn’t a statistically significant correlation between these two variables.\n\n\n\n\n\n\nNote\n\n\n\nThis implies that the education levels among younger and older residents were similar. Given that education often relates to income, this discovery might indicate limited social mobility among the city’s residents. Consequently, we’ll delve into examining the connection between education and wage next\n\n\n\nPlotCodeNormality TestCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = final_data,\n  x = educationLevel, \n  y = age,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 7),\n        legend.position = \"none\") +\n  labs(x = \"Education Level\", y = \"Age\") +\n  ggtitle(\"Age by Education Level\")\n\n\n\n\n\n\n\n\n\n\n\n# Calculate Shapiro-Wilk test statistic and p-value\nsw_test <- shapiro.test(final_data$age)\nsw_stat <- sw_test$statistic\nsw_p <- sw_test$p.value\n\nggplot(final_data,\n       aes(sample=age)) +\n  stat_qq() +\n  stat_qq_line() +\n  annotate(\"text\", x = -1.5, y = -2.5, \n           label = paste(\"Shapiro-Wilk test:\", \"\\n\", \"statistic =\", round(sw_stat, 3), \"\\n\", \"p-value =\", format(sw_p, scientific = TRUE, digits = 3)), \n           hjust = 0, vjust = 0, size = 3, color = \"black\")+\n  ggtitle(\"Distribution - Age\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  labs(y = \"Age\")"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#education-and-income",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#education-and-income",
    "title": "Demographic Visual Analysis",
    "section": "6.3 Education and Income",
    "text": "6.3 Education and Income\nTo further explore on income, let’s create a new variable mean_wage, which is calculated by using rowMeans() to find the average of all wage related columns.\n\n\nCode\n# Select the columns that contain wage information\nwage_cols <- grepl(\"^Wage\", names(final_data))\n\n# Calculate the average wage\nfinal_data$mean_wage <- rowMeans(final_data[, wage_cols], na.rm = TRUE)\n\n\nBefore proceeding with the confirmed data analysis, a prior check similar to earlier assessments was conducted to test the normality assumption for the average wage distribution among residents. The observed departure from normality in the distribution of average wages, as evident in the QQ plot, led to the decision to utilize a non-parametric test.\nAnalysis of the boxplot unveiled noticeable variations in residents’ wages across different education levels. Notably, individuals with higher educational attainment tended to earn considerably higher wages.\n\n\n\n\n\n\nNote\n\n\n\nEducation was found to have a positive correlation with wages. However, in section 6.2, it was observed that education levels across different age groups were not significantly different. To enhance citizens’ wages, the City of Engagement can consider implementing strategies to encourage the younger generation to pursue further studies.\n\n\n\nPlotCodeNormality TestCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = final_data,\n  x = educationLevel, \n  y = mean_wage,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 7),\n        legend.position = \"none\") +\n  labs(x = \"Education Level\", y = \"Income\") +\n  ggtitle(\"Income by Education Level\")\n\n\n\n\n\n\n\n\n\n\n\n# Select the columns that contain wage information\nwage_cols <- grepl(\"^Wage_20\", names(final_data))\n\n# Calculate the average wage\nfinal_data$mean_wage <- rowMeans(final_data[, wage_cols], na.rm = TRUE)\n\n# Calculate Shapiro-Wilk test statistic and p-value\nsw_test <- shapiro.test(final_data$mean_wage)\nsw_stat <- sw_test$statistic\nsw_p <- sw_test$p.value\n\nggplot(final_data,\n       aes(sample=mean_wage)) +\n  stat_qq() +\n  stat_qq_line() +\n  annotate(\"text\", x = -1.5, y = -2.5, \n           label = paste(\"Shapiro-Wilk test:\", \"\\n\", \"statistic =\", round(sw_stat, 3), \"\\n\", \"p-value =\", format(sw_p, scientific = TRUE, digits = 3)), \n           hjust = 0.5, vjust = -1.5, size = 3, color = \"black\")+\n  ggtitle(\"Distribution - Wage\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n       axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  labs(y = \"Wage\")"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#overall-financial-health",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#overall-financial-health",
    "title": "Demographic Visual Analysis",
    "section": "7.1 Overall Financial Health",
    "text": "7.1 Overall Financial Health\nIn order to analyze earning and spending overtime. A new dataframe named ‘Earn_Exp_avg’ with the following variables will be created:\n\nMonth: Date in ‘YYYY-MM’ format\nEducationLevel: No change from original data\nAvg_Earning: Average monthly earnings i.e., both wage and rentadjustments\nAvg_Expenditure: Average monthly expenditure\nAvg_Savings: Calculated variable deducting monthly expenditure from monthly earnings\n\n\n\nCode\n#Transform Earning Data to long form\nEarn_data_long <- final_data %>%\n  select(participantId, educationLevel, starts_with(\"TotEarn\")) %>%\n  gather(key = \"Month\", value = \"Earning\", starts_with(\"TotEarn\"))\n\nEarn_data_long_renamed <- Earn_data_long %>% \n  mutate(Month = case_when(\n    Month == \"TotEarn_Mar\" ~ \"2022-03\",\n    Month == \"TotEarn_Apr\" ~ \"2022-04\",\n    Month == \"TotEarn_May\" ~ \"2022-05\",\n    Month == \"TotEarn_Jun\" ~ \"2022-06\",\n    Month == \"TotEarn_Jul\" ~ \"2022-07\",\n    Month == \"TotEarn_Aug\" ~ \"2022-08\",\n    TRUE ~ Month\n  )) %>% \n  rename(Month_renamed = Month)\n\nEarn_data_long_renamed$Month <- as.Date(paste0(Earn_data_long_renamed$Month, \"-01\"), format = \"%Y-%m-%d\")\n\nEarn_data_long_renamed <- as_tibble(Earn_data_long_renamed)\n\n#Transform Exp Data to long form\nExp_data_long <- final_data %>%\n  select(participantId, educationLevel, starts_with(\"TotExp\")) %>%\n  mutate(across(starts_with(\"TotExp\"), ~coalesce(., 0))) %>%\n  gather(key = \"Month\", value = \"Expenditure\", starts_with(\"TotExp\"))\n\nExp_data_long_renamed <- Exp_data_long %>% \n  mutate(Month = case_when(\n    Month == \"TotExp_Mar\" ~ \"2022-03\",\n    Month == \"TotExp_Apr\" ~ \"2022-04\",\n    Month == \"TotExp_May\" ~ \"2022-05\",\n    Month == \"TotExp_Jun\" ~ \"2022-06\",\n    Month == \"TotExp_Jul\" ~ \"2022-07\",\n    Month == \"TotExp_Aug\" ~ \"2022-08\",\n    TRUE ~ Month\n  )) %>% \n  rename(Month_renamed = Month)\n\nExp_data_long_renamed$Month <- as.Date(paste0(Earn_data_long_renamed$Month, \"-01\"), format = \"%Y-%m-%d\")\n\nExp_data_long_renamed <- as_tibble(Exp_data_long_renamed)\n#sum(is.na(Exp_data_long_renamed$Expenditure))\n\n# Group and summarize earning data by month\nEarn_data_avg <- Earn_data_long_renamed %>% \n  group_by(Month, educationLevel) %>% \n  summarize(avg_earning = mean(Earning))\n\n# Group and summarize expenditure data by month\nExp_data_avg <- Exp_data_long_renamed %>% \n  group_by(Month, educationLevel) %>% \n  summarize(avg_expenditure = mean(Expenditure))\n\nEarn_Exp_avg <- full_join(Earn_data_avg, Exp_data_avg, \n                           by = c(\"educationLevel\", \"Month\"))\n\n# Create a new column for savings\nEarn_Exp_avg$savings <- Earn_Exp_avg$avg_earning - Earn_Exp_avg$avg_expenditure\n\n#Round values to 2dp\nEarn_Exp_avg$avg_expenditure <- round(Earn_Exp_avg$avg_expenditure, 2)\nEarn_Exp_avg$avg_earning <- round(Earn_Exp_avg$avg_earning, 2)\nEarn_Exp_avg$savings <- round(Earn_Exp_avg$savings, 2)\n\n#Rename Columns \nEarn_Exp_avg <- Earn_Exp_avg %>% rename(Education_Level = educationLevel, Avg_Earning = avg_earning, Avg_Expenditure = avg_expenditure, Avg_Savings = savings)\n\n\nThe charts below illustrate that residents’ monthly expenditures remained relatively constant across different education levels throughout the months. Despite an earnings surge in March, expenditure levels remained notably low. It’s worth highlighting that individuals with higher wages tended to save more monthly, as indicated in the highlighted yellow section, rather than increasing their spending.\n\n\n\n\n\n\nNote\n\n\n\nThis shows a good sign of financial health across all education levels.\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# Create a ggplot object with the data\np <-ggplot(data = Earn_Exp_avg, aes(x = Month)) +\n  geom_line(aes(y = Avg_Earning, color = \"Avg_Earning\")) +\n  geom_line(aes(y = Avg_Expenditure, color = \"Avg_Expenditure\")) +\n  geom_ribbon(aes(ymin = Avg_Expenditure, ymax = Avg_Earning), fill = \"yellow\", alpha = 0.3) +\n  scale_color_manual(name = NULL, values = c(\"Avg_Earning\" = \"steelblue\", \"Avg_Expenditure\" = \"orange\"), \n                     labels = c(\"Average Earnings\", \"Average Expenditure\")) +\n  labs(title = \"Average Earnings and Expenditure per Month\",\n       x = \"Month\",\n       y = \"Amount\") +\n  theme_classic() +\n  facet_wrap(~ Education_Level, ncol = 1, scales = \"free_y\") +\n  ylim(0, 10000) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 10),\n        axis.text = element_text(size = 6))\n\n# Convert the ggplot object to an interactive plotly object and modify the hoverinfo argument\np <- ggplotly(p, height = 500, tooltip = c(\"Avg_Earning\", \"Avg_Expenditure\")) %>%\n  layout(hoverlabel = list(bgcolor = \"white\"))\n\n# Set the legend position to \"bottom\"\np <- layout(p, legend = list(orientation = \"h\", x = 0.25, y = -0.1))\n\n# Display the plot\np"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#spending-pattern",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#spending-pattern",
    "title": "Demographic Visual Analysis",
    "section": "7.2 Spending Pattern",
    "text": "7.2 Spending Pattern\nTo further examine on spending, let’s create a new dataframe called spending_summary:\n\nEducation\nFood\nRecreation\nShelter\nMean Spending\n\n\n\nCode\n# Create New variable for total average education expenditure\nfinal_data$Education <- rowMeans(final_data[c(\"Education_2022-03\", \"Education_2022-04\", \"Education_2022-05\",\"Education_2022-06\", \"Education_2022-07\", \"Education_2022-08\")])\n\n# Create New variable for total average food expenditure\nfinal_data$Food<- rowMeans(final_data[c(\"Food_2022-03\", \"Food_2022-04\", \"Food_2022-05\",\"Food_2022-06\", \"Food_2022-07\", \"Food_2022-08\")])\n\n# Create New variable for total average recreation expenditure\nfinal_data$Recreation<- rowMeans(final_data[c(\"Recreation_2022-03\", \"Recreation_2022-04\", \"Recreation_2022-05\",\"Recreation_2022-06\", \"Recreation_2022-07\", \"Recreation_2022-08\")])\n\n# Create New variable for total average shelter expenditure\nfinal_data$Shelter<- rowMeans(final_data[c(\"Shelter_2022-03\", \"Shelter_2022-04\", \"Shelter_2022-05\",\"Shelter_2022-06\", \"Shelter_2022-07\", \"Shelter_2022-08\")])\n\n# Round the result to 2 decimal places\nfinal_data$Education <- round(final_data$Education, 2)\nfinal_data$Food <- round(final_data$Food, 2)\nfinal_data$Recreation <- round(final_data$Recreation, 2)\nfinal_data$Shelter <- round(final_data$Shelter, 2)\n\n# Calculate the mean spending for each category\nmean_spending <- c(mean(final_data$Education), mean(final_data$Food), \n                   mean(final_data$Recreation), mean(final_data$Shelter))\n\n# Create a data frame with the mean spending for each category\nspending_summary <- data.frame(Category = c(\"Education\", \"Food\", \"Recreation\", \"Shelter\"),\n                               Mean_Spending = mean_spending)\n\n\nFrom the bar chart below, we see that a approximately half of total monthly expenditure was spent on shelter. This was followed by recreational activities, then food and lastly education.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n# Create a bar chart of the mean spending for each category\nspending_summary %>%\n  mutate(Category = reorder(Category, -Mean_Spending)) %>%\n  ggplot(aes(x = Category, y = Mean_Spending)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = paste0(\"$\", round(Mean_Spending, 2))), vjust = -0.5) +\n  ggtitle(\"Average Monthly Spending by Category\") +\n  xlab(\"Category\") +\n  ylab(\"Average Monthly Spending\") +\n  theme_classic() +  \n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 10),\n        axis.text = element_text(size = 10))"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#financial-health-and-joviality",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#financial-health-and-joviality",
    "title": "Demographic Visual Analysis",
    "section": "8.1 Financial Health and Joviality",
    "text": "8.1 Financial Health and Joviality\nLinking financial well-being to happiness seems reasonable. Let’s investigate the hypothesis that these variables have a positive correlation—that higher wealth or more savings correspond to greater happiness. However, the scatter plot presents a contrasting picture. Specifically, in the top-left corner, instances of residents with substantial savings exhibit low scores in joviality, indicating a contradiction to the expected positive relationship between savings and happiness.\n\n\n\n\n\n\nNote\n\n\n\nIn contrast to our initial hypothesis, there was a negative correlation discovered between monthly savings and joviality. This unexpected finding might be attributed to external stressors associated with higher-paying jobs. It could prove beneficial for the local council to engage with high earners in the city to offer more comprehensive support for their overall well-being.\n\n\n\nPlotCodeStatistical TestCode\n\n\n\n\n\n\n\n\n\n\n\n# Create New variable for Savings\nfinal_data$Avg_Savings <- rowMeans(final_data[, c(\"TotSav_Mar\", \"TotSav_Apr\", \"TotSav_May\",\"TotSav_Jun\", \"TotSav_Jul\", \"TotSav_Aug\")])\n\n# Round the result to 2 decimal places\nfinal_data$Avg_Savings <- round(final_data$Avg_Savings, 2)\n\nplot_ly(data = final_data, \n        x = ~joviality, \n        y = ~Avg_Savings,\n        color = ~educationLevel,\n        type = \"scatter\",\n        mode   = 'markers') %>%\n  add_trace(\n    text = ~paste(\"Joviality: \", joviality, \"<br>\",\n                  \"Avg Savings: $\", Avg_Savings),\n    hoverinfo = \"text\",\n    showlegend = FALSE\n  ) %>%\n  layout(\n    title = \"Correlation between Joviality and Savings by Education\",\n    xaxis = list(title = \"Joviality Score\"),\n    yaxis = list(title = \"Average Savings\"),\n    margin = list(l = 60, r = 10, t = 60, b = 30),\n    plot_bgcolor = \"white\",\n    paper_bgcolor = \"white\",\n    font = list(color = \"black\"),\n    hoverlabel = list(bgcolor = \"white\", font = list(color = \"black\")),\n    legend = list(title = \"Education Level\", font = list(color = \"black\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Avg_Savings,\n  marginal = FALSE,\n  ) + \nggtitle(\"Correlation betwen Joviality and Savings\") +\nlabs(x = \"Joviality Score\", y = \"Average Savings\")"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#recreation-and-joviality",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#recreation-and-joviality",
    "title": "Demographic Visual Analysis",
    "section": "8.2 Recreation and Joviality",
    "text": "8.2 Recreation and Joviality\nOther than financial stability, finding fulfillment in activities outside of work could be a significant contributor to happiness. To delve into this, we’re using expenditure on recreational activities as an indicator. This helps us examine whether individuals who allocate more towards recreational spending tend to report higher scores in joviality.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Recreation,\n  marginal = FALSE,\n  ) + \nggtitle(\"Correlation betwen Joviality and Recreation\") +\nlabs(x = \"Joviality Score\", y = \"Recreation Expenditure\")\n\n\n\n\nBreaking down this relationship further by interest groups, we also observe that this positive correlation is true, across all groups.\n\n\n\n\n\n\nNote\n\n\n\nThis discovery indicates that active involvement in interest groups can lead to happier and more fulfilling lives. Hence, it’s crucial for the local council to focus on this aspect. For instance, understanding residents’ preferences and identifying existing infrastructure gaps can aid in better resource allocation and encourage increased participation in these activities.\n\n\n\nPlotData\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = joviality,\n  y = Recreation,\n  marginal = FALSE,\n  ) + \n  ggtitle(\"Correlation between Joviality and Recreation Expenditure by Interest Groups\") +\n  labs(x = \"Joviality Score\", y = \"Recreation Expenditure\") +\n  facet_wrap(~interestGroup)"
  },
  {
    "objectID": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#age-and-joviality",
    "href": "Data_Analytics_Projects/Population Visual Analysis/Population_Visual_Analysis.html#age-and-joviality",
    "title": "Demographic Visual Analysis",
    "section": "8.3 Age and Joviality",
    "text": "8.3 Age and Joviality\nUpon reviewing the scatter plot below, a slight negative correlation seems apparent between age and joviality.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = final_data,\n  x = age,\n  y = joviality,\n  marginal = FALSE,\n  )\n\n\n\n\nUpon closer examination of the distribution of joviality scores categorized by 10-year age groups, a prominent peak is observable among residents in their 50s, suggesting a concentration of scores around 0.4 and below. Subsequently, this distribution gradually decreases as joviality scores increase.\n\n\n\n\n\n\nNote\n\n\n\nGiven that older residents have been significant contributors to the community, the data strongly implies a need for further investigation into the factors contributing to their lower joviality scores. Addressing the well-being of older residents not only ensures their proper care but also provides reassurance to younger residents, knowing that their parents are well looked after and that similar support will be available to them in the future.\n\n\n\nDistributionCode\n\n\n\n\n\n\n\n\n\n\nggplot(final_data, \n       aes(x = joviality, \n           y = age_band,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"Joviality Scores\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon Chiu",
    "section": "",
    "text": "Education\n\nSingapore Management University, Master of IT in Business (Data Analytics Track)\nNational Taiwan University, Bachelor of International Business\n\n\n\nExperience\n\nIHH Healthcare, Business Data Analyst Intern:\n\n\nConstructed Power BI ETL pipeline and dashboards\nDesigned Power Apps applications\nAnalyzed clinical data\n\n\nShopee, Digital Advertising Analyst Intern:\n\n\nPerformed online advertising on Shopee platform\nMentored over 100 clients in online advertising\n\n\nClarence Language Network, Project Manager Intern:\n\n\nInitiated SEO projects\nAnalyzed GA data\nManaged company website"
  },
  {
    "objectID": "PowerBI_tutorial_articles/Jira_with_PowerBI/Jira_with_PowerBI.html",
    "href": "PowerBI_tutorial_articles/Jira_with_PowerBI/Jira_with_PowerBI.html",
    "title": "Jira_with_PowerBI",
    "section": "",
    "text": "2. Generate Jira Token\nWe need to first get the Jira token to connect to Power BI.\nGo to Manage API to generate Jira Token. The token won’t expire unless you revoke it.\n\n\n\n3. Convert Token to Base64 format\nAfter getting the token, the next step is to convert it to base64 format.\nThis can be done in two ways:\n\nUse this excel file Base64Encode.xlsm (Reference: Encode Base64 in Excel VBA - YouTube).\nUse this Python code jira_api_turtorial.ipynb.\n\n\n\n4. Import Data into Power BI\nOne limitation of using Jira API to ingest data is a single API call restricts to a maximum of 100 subsequent records to be retrieved. To retrieve all records, multiple smaller requests are made using the API and Pagination. (Reference). Following steps will show you how to do it step by step.\n\nSelect ‘Transform Data’ to open the Power Query Editor. (Reference)\n\nCreate a “New Group” and name it “Jira”\n\nIn the Jira group (folder) create a Blank Query and then right click it to open the query in the ‘Advanced Editor’.\n\nReplace the code with this code in the Advanced Editor.\n\nAfter replacing the code, we need to do some modifications.\nFirst, replace ihhgroupdata with your company domain name.\n\nSecond, replace the Base64_here with the base64 encoded string that was generated in Section 3. Remember to keep the Basic in front of it. (Make sure you keep a space after Basic) \nThird, replace IHHITREQ (E.g. Or your JIRA Project) with your project key. (Your key can be found in your Jira website.) If you want to further include more projects, you can do like this: project in ('IHHITREQ','project1','project2')\n\n\nFourth, you can also change the query to your desired data range by modifying this part.\n\nWhen done rename the query to FetchPage.\nCreate a second query and rename it to FetchPages and copy the following code into it.\nIf an error pops up with the following message ignore it — this will be solved after creating (and renaming) the third query: Expression.Error: The name ‘GenerateByPage’ wasn’t recognized. Make sure it’s spelled correctly.\n\nCreate a third Query and rename it to GenerateByPage .\nThe result queries so far should be:\n\nFetchPage\nFetchPages\nGenerateByPage\n\nThe final step before the data can be used in PowerBI is to invoke the ‘FetchPage’ query. Input 50 in pageSize window and click Invoke button.\n\nIf this error pops up, please click Edit Credential.\n\nAnd then use this setting to connect.\n\nAfter invoking the query, query result should pop up. Based on your need, you can expand columns and do your own data transformation. \n\n\n\n\n5. Publish and Set Schedule Refreshment\nAfter you finish your dashboard and publish it, you may encounter this refreshment problem.\n\n\nTo solve it, click on the three dots and select Settings.\n\nFind the Data source credentials option and click Edit credentials.\n\nA window will pop up. Fill in the windows like below and click Sign in.\n\nFind the Refresh option, which is located under the Data Source Credentials option we just used. Turn on the Refresh option and configure it based on your needs.\n\nAfter applying the settings, try refreshing your report again. It should work fine now."
  },
  {
    "objectID": "PowerBI_tutorial_articles/Jira_with_PowerBI/resource/jira_api_turtorial.html",
    "href": "PowerBI_tutorial_articles/Jira_with_PowerBI/resource/jira_api_turtorial.html",
    "title": "Simon Chiu",
    "section": "",
    "text": "1. Get issues from certain project\nThe first link is a video teaching you how to write the code. Noted that the api version in this video is v2.\nhttps://www.youtube.com/watch?v=3POeV_RcKuw&list=PLI8raxzYtfGwe8f-8s8O69gBVd9JIr9dM&index=2\nThe second link here is the jira documentation. Noted that the api version here is v3.\nhttps://developer.atlassian.com/cloud/jira/platform/rest/v3/api-group-issue-search/#api-rest-api-3-search-get\nBelow code is a more flexible version of the combination of these two links.\n\nimport requests\nfrom requests.auth import HTTPBasicAuth\nimport json\n\nur_domain = \"company_name\" #replace with ur company's name. You can refer to ur jira homepage url\nurl = f\"https://{ur_domain}.atlassian.net/rest/api/3/search\" #we're using jira api v3\n\nur_email = \"xxx@bbb.com\" #replace with ur email\napi_token = 'token' #ur own token can get from https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/\nauth = HTTPBasicAuth(ur_email, api_token)\n\nheaders = {\n  \"Accept\": \"application/json\"\n}\n\n#Replace project_name with ur project's name\n#To find the project name, key the url above to ur browser and then u will get a json type html page. Ctrl+f to search for \"project\".\n#Inside \"project\" dictionary, u may find a field called \"key\". The value of the \"key\" is ur project name.\n#It may look similar like the project name you see in the jira website but not exactly the same.\nproject_name = \"project\" \n# This is a jql query to filter out specific project's issues\nquery = {\n  'jql': f'project = {project_name}'\n}\n\nresponse = requests.request(\n   \"GET\",\n   url,\n   headers=headers,\n   params=query,\n   auth=auth\n)\n\n# parsed_json is a dict, can be used for further fields selection or filtering\nparsed_json = json.loads(response.text)\nprint(type(parsed_json))\n\n# json.dumps gives a clear output formate to read.\n# below prints the information for the first issue\nprint(json.dumps(parsed_json[\"issues\"][0], sort_keys=True, indent=4, separators=(\",\", \": \")))\n\n\n\n2. Transform jira token to Base64\n\nimport base64\nyour_email = 'xxx@ihhhealthcare.com' #replace this with your own email\noriginal_token = 'abc' #replace this with your jira token\ncombined_string = your_email+':'+original_token\nb = base64.b64encode(bytes(combined_string, 'utf-8')) # bytes\nprint(b)"
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article1/Quarto_Article1.html",
    "href": "Quarto_tutorial_articles/Quarto_Article1/Quarto_Article1.html",
    "title": "How to build a Quarto website",
    "section": "",
    "text": "Step1: Install Git\nFirst, download Git.\nIf you already have Git, you can skip this step.\n\n\nStep2: Create a GitHub Account\nCreate GitHub account.\nThere are two things to note:\n\nUse your permanent email to create the account; don’t use your work or school account. This is to ensure you can always access your account and work.\nUse a simple GitHub username that doesn’t include spaces in the name.\n\nIf you already have a GitHub account, you can skip this step.\n\n\nStep3: Install RStudio\nInstall Rstudio (now it has a new name Posit).\nIf you already have Rstudio, you can skip this step.\n\n\nStep4: Setup RStudio\n\nOpen your RStudio. Click “File” and select “New Project” within Rstudio.\n\nSelect “New Directory”.\n\nSelect “Quarto Website”. (If there is no such option, you may need to update your Rstudio version. )\n\nFor “Directory name”, don’t include any space in the name.\n\nFor “Create projects as a subdirectory of”, click on “Browse” to select the folder with the same name as your GitHub username. If you haven’t created a folder with this name yet, please create one first. For instance, my GitHub username is “simonchiu902”, so I created a folder named “simonchiu902” and then used “Browse” to select this folder.\n\nMake sure other settings are the same as the screenshot and click on “Create Project”.\nAfter the above steps, Rstudio will direct you to your new project.\n\nIf you check your directory, you’ll find the following files have been created.\n\nNow, go back to your Rstudio and click “Build” and “Render Website”.\n\nA window will pop up and this is the draft of your website where we will further make changes.\n\n\n\n\nStep5: Set up Git\nWe are going to use Git within R for version control.\n\nType in “git config –global user.name” in the “Terminal” to check if you’re linking to the correct Github name.\n\nIf your output is empty or wrong, type “git config --global user.name”your_username”” in your terminal. (replace your_username with your correct username)\n\nType in “git config --global user.email” to check if the email is correct.\n\nIf not, then type “git config --global user.email”your_email”” in your terminal. (replace your_email with your correct email)\n\nClick “Tools” and “Install Packages” to install “devtools” and “usethis”\n\n\n\n\n\nType “usethis::use_git()” in your “Console”. (note that this time we use “Console”, not “Terminal” in previous steps)\n\nIt will ask you “Is it ok to commit them?”. We don’t want to commit now so select “No” by typing the corresponding number.\n\nThen it will ask you “A restart of RStudio is required to activate the Git pane. Restart now?”. Select “Yup” by entering the corresponding number.\n\nAfter restart, you can see a “Git” option is added on the upper right panel.\n\nCheck all items in the Git panel.\n\n\n\n\n\n\n\nTip\n\n\n\nclick on first item, press Shift and click the last item to select all items\n\n\n\n\n\nAfter selecting all items, click “Commit”.\n\nThe window will pop up. Fill in your desired comment in the “Commit message” and then click “Commit”. \nAfter “Commit”, go back to your Rstudio. There should be no more items under “Git”.\n\n\n\n\n\nStep6: Build Link to GitHub\n\nType “usethis::use_github()” in the Console. \nIf you encounter the following error, this may be because you lack a valid Github token. To solve this, please follow the following steps.\n\nType “usethis::create_github_token()” and this will direct you to a Github page.\n\nFill in your “Note” and set up “Expiration” days.\n\nScroll to the bottom of the page and click “Generate token”. This will direct you to a new page with a new generated token. Do not close this new page with your token, we’ll use it later.\n\nBack to your Rstudio, type “gitcreds::gitcreds_set()”.\n\nIn my case, I previously used some older and outdated credentials (tokens), triggering this message. I’ll proceed by choosing “Replace these credentials” and typing the associated number. If you haven’t entered any credentials previously, you might simply be prompted to input your new credentials directly into the console, allowing you to paste your newly generated credential.\n\nAfter I enter 2, it prompts me to enter my newly generated token. You can go back to the Github page showing the new token, copy and paste it in the console.\n\nAfter successfully render your token, type in “usethis::use_github()” again.\n\nThis time it should successfully direct you to Github and create a new repository for you.\n\n\n\n\nStep7: Use Netlify to publish your website\n\nSign up Netlify with your Github account.\n\nAfter signing in, it will direct you to this page.\n\nFind the “Add new site” button and click on “Import an existing project”.\n\nClick “Deploy with GitHub” and link Netify with your Github account.\n\nSelect your newly created repository.\n\nKeep all the settings unchanged, but specifically input “_site” into the “Publish directory” field. Click on “Deploy” to finish the setting.\n\nAfter you deploy the website, it will direct you to this page. The link is a random name and we can change it by clicking “Site configuration”.\n\nFind “Change site name” and change the url to your desired name.\n\n\nYou may need to wait a few minutes for Netify to process and then you can use the new URL to access your website.\n\n\n\nCredit\nI would like to acknowledge Dr. KAM Tin Seong for his guidance on utilizing Quarto, imparted during his Visual Analytics course at SMU. This tutorial is compiled based on his lectures, and I believe it’s valuable to share with individuals keen on exploring Quarto."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html",
    "title": "Custom Quarto Website",
    "section": "",
    "text": "In this article, I will demonstrate how to customize your Quarto website, which was created following the tutorial in the initial article. If you haven’t read the first article, please access it through this link."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#title",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#title",
    "title": "Custom Quarto Website",
    "section": "3-1 Title",
    "text": "3-1 Title\nThe following two pictures illustrate how the code chunk is linked to a specific title section of the website.\n\n\nLet’s modify the code to “Simon Chiu Personal Website”.\n\nTo observe how this code alteration impacts the website, click on “Preview Website” within the “Build” section. Note that this change hasn’t been pushed to Github, it’s just a preview. We’ll cover how to save the change permanently at the end of this article.\n\nAfter clicking the “Preview Website” button, your updated preview website will pop up. The title of the website correspondingly reflects this change."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#navigation-bar",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#navigation-bar",
    "title": "Custom Quarto Website",
    "section": "3-2 Navigation Bar",
    "text": "3-2 Navigation Bar\n\n\nWe have the option to partition the navigation bar into two sections: left and right. Initially, we’ll concentrate on the left part. The first step involves establishing two primary labels: “Data Analytics Project” and “Quarto Tutorial Articles.” Beneath each primary label, we’ll create a corresponding sub-label (“Project 1” and “Article 1”). It’s important to note that the “href” attribute is utilized to link to specific files, necessitating the creation of Project 1.html and Article 1.html files. Detailed guidance on file creation will be provided in subsequent tutorial articles. Presently, our focus is solely on outlining the navigation bar structure.\n\n\nAs for the right part, we include two qmd files: index.qmd and about.qmd. Note that if we don’t include text:\"Home\" then the name of index.qmd shown on the website will be default to index."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#theme",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#theme",
    "title": "Custom Quarto Website",
    "section": "3-3 Theme",
    "text": "3-3 Theme\n\nThere are multiple built-in themes you can choose from Quarto - HTML Theming. \nFor example, if we change “cosmo” to “journal”, the website will look like this.\n\nIf you don’t like the theme’s navbar color, you can change the color by adding “background”."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#icon",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#icon",
    "title": "Custom Quarto Website",
    "section": "3-4 Icon",
    "text": "3-4 Icon\nIt’s common to include icons that link to your other profiles. For instance, you can enhance your navbar by incorporating GitHub and LinkedIn icons. Achieve this by inserting the following four lines of code into the right section of the navbar."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#image",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#image",
    "title": "Custom Quarto Website",
    "section": "4-1 Image",
    "text": "4-1 Image\nFirst, we have mentioned Title in 3-1, so let’s just start from image.\nTo insert a image, the first step is to create an “images” folder and place all the images you intend to use within this folder for better organization.\nNote that the directory of this “images” folder should match the location of the index.qmd file for the images to be properly referenced.\n\nThen put your picture into this “images” folder.\n\nTo reference this picture, we use the relative path: \"images/icon.png\" By doing so, the website can show the picture successfully."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#about-template",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#about-template",
    "title": "Custom Quarto Website",
    "section": "4-2 about: template",
    "text": "4-2 about: template\nThere are various kinds of templates can choose via Quarto official website.\nEven though these template were designed for about.qmd, I find that it’s also applicable to the index.qmd.\nIn my case, I choose trestles, which will make the outfit of index.qmd look like this:"
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#format-html-fontsize",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#format-html-fontsize",
    "title": "Custom Quarto Website",
    "section": "4-3 format: html: fontsize",
    "text": "4-3 format: html: fontsize\nIf you observe the RStudio user interface, there isn’t an option to directly adjust font sizes within the text editor. You can only change the text format from normal to various header styles.\n\nWhat if the font size of the ‘Normal’ style is still too large?\nThat’s why we use format: html: fontsize to adjust the fontsize of “Normal”."
  },
  {
    "objectID": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#content",
    "href": "Quarto_tutorial_articles/Quarto_Article2/Quarto_Article2.html#content",
    "title": "Custom Quarto Website",
    "section": "4-4 Content",
    "text": "4-4 Content\nWe’ve completed the explanation of the top section of index.qmd. Let’s now proceed to discuss the main content.\n\nThere are two ways to edit your main content. First is using the Visual Editor. This provides you a more user-friendly UI interface.\n\nAlternatively, if you are familiar with markdown of Jupyter Notebook, you can choose the Source Editor.\nYou can find some common markdown options via this link."
  }
]